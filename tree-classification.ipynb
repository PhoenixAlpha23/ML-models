{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11559800,"sourceType":"datasetVersion","datasetId":7248111}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:09:05.955273Z","iopub.execute_input":"2025-04-25T09:09:05.955516Z","iopub.status.idle":"2025-04-25T09:09:08.161538Z","shell.execute_reply.started":"2025-04-25T09:09:05.955491Z","shell.execute_reply":"2025-04-25T09:09:08.160567Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom shutil import copyfile\n\ndef augment_image(image_path, output_dir):\n    img = cv2.imread(image_path)\n    if img is None:\n        return  # Skip corrupted files\n    h, w = img.shape[:2]\n    filename = os.path.splitext(os.path.basename(image_path))[0]\n    \n    # Save original\n    cv2.imwrite(os.path.join(output_dir, f\"{filename}.jpg\"), img)\n    \n    # Standard rotations (90°, 180°, 270°)\n    for angle in [90, 180, 270]:\n        rotated = np.rot90(img, k=angle // 90)\n        cv2.imwrite(os.path.join(output_dir, f\"{filename}_rot{angle}.jpg\"), rotated)\n    \n    # Tilted rotations (30°, 45°)\n    for angle in [30, 45]:\n        M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1.0)\n        rotated = cv2.warpAffine(img, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n        cv2.imwrite(os.path.join(output_dir, f\"{filename}_tilt{angle}.jpg\"), rotated)\n    \n    # Horizontal flip\n    flipped = cv2.flip(img, 1)\n    cv2.imwrite(os.path.join(output_dir, f\"{filename}_flip.jpg\"), flipped)\n\n# Step 1: Augment all images\nbase_dir = \"/kaggle/input/tree-10/Tree_classification\"\naugmented_dir = \"Tree_classification_augmented\"\nos.makedirs(augmented_dir, exist_ok=True)\n\nfor class_name in os.listdir(base_dir):\n    class_dir = os.path.join(base_dir, class_name)\n    augmented_class_dir = os.path.join(augmented_dir, class_name)\n    os.makedirs(augmented_class_dir, exist_ok=True)\n    \n    for img_file in os.listdir(class_dir):\n        img_path = os.path.join(class_dir, img_file)\n        augment_image(img_path, augmented_class_dir)\n\n# Step 2: Split into train/val/test (70/20/10)\nfor class_name in os.listdir(augmented_dir):\n    class_dir = os.path.join(augmented_dir, class_name)\n    if not os.path.isdir(class_dir):\n        continue  # Skip non-directory files\n    \n    images = [f for f in os.listdir(class_dir) if f.endswith(('.jpg', '.png'))]\n    if not images:\n        continue  # Skip empty folders\n    \n    # Split 70% train, 20% val, 10% test\n    train_files, temp_files = train_test_split(images, test_size=0.3, random_state=42)\n    val_files, test_files = train_test_split(temp_files, test_size=0.333, random_state=42)\n    \n    # Create split directories and copy files\n    for split, files in [(\"train\", train_files), (\"val\", val_files), (\"test\", test_files)]:\n        split_dir = os.path.join(augmented_dir, split, class_name)\n        os.makedirs(split_dir, exist_ok=True)\n        for f in files:\n            src = os.path.join(class_dir, f)\n            dst = os.path.join(split_dir, f)\n            copyfile(src, dst)\n\nprint(\"Augmentation and splitting complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:09:36.508050Z","iopub.execute_input":"2025-04-25T09:09:36.508934Z","iopub.status.idle":"2025-04-25T09:09:44.260663Z","shell.execute_reply.started":"2025-04-25T09:09:36.508902Z","shell.execute_reply":"2025-04-25T09:09:44.259777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from skimage.feature import graycomatrix, graycoprops\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\ndef preprocess_image(image_path):\n    img = cv2.imread(image_path)\n    if img is None:\n        return None\n    \n    # Only contrast enhancement (no resizing)\n    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n    l, a, b = cv2.split(lab)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    l_clahe = clahe.apply(l)\n    lab_clahe = cv2.merge((l_clahe, a, b))\n    img_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n    \n    return img_clahe  # Still 256x256\n\ndef extract_features(img):\n    if img is None:\n        return None\n    \n    # Color Histogram (RGB)\n    hist_r = cv2.calcHist([img], [0], None, [256], [0, 256]).flatten()\n    hist_g = cv2.calcHist([img], [1], None, [256], [0, 256]).flatten()\n    hist_b = cv2.calcHist([img], [2], None, [256], [0, 256]).flatten()\n    \n    # Texture (Haralick via GLCM)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    glcm = graycomatrix(gray, distances=[1], angles=[0], levels=256, symmetric=True, normed=True)\n    contrast = graycoprops(glcm, 'contrast')[0, 0]\n    energy = graycoprops(glcm, 'energy')[0, 0]\n    \n    # Combine features\n    features = np.concatenate([hist_r, hist_g, hist_b, [contrast, energy]])\n    return features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:29:39.184046Z","iopub.execute_input":"2025-04-25T09:29:39.184383Z","iopub.status.idle":"2025-04-25T09:29:39.192841Z","shell.execute_reply.started":"2025-04-25T09:29:39.184329Z","shell.execute_reply":"2025-04-25T09:29:39.192089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_data(root_dir):\n    X, y = [], []\n    class_names = sorted(os.listdir(root_dir))\n    label_map = {name: idx for idx, name in enumerate(class_names)}\n    \n    for class_name in class_names:\n        class_dir = os.path.join(root_dir, class_name)\n        for img_file in os.listdir(class_dir):\n            img_path = os.path.join(class_dir, img_file)\n            img = preprocess_image(img_path)\n            if img is not None:\n                features = extract_features(img)\n                X.append(features)\n                y.append(label_map[class_name])\n    \n    return np.array(X), np.array(y), label_map\n\nX_train, y_train, label_map = load_data(\"/kaggle/working/Tree_classification_augmented/train\")\nX_val, y_val, _ = load_data(\"/kaggle/working/Tree_classification_augmented/val\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:29:42.497705Z","iopub.execute_input":"2025-04-25T09:29:42.497981Z","iopub.status.idle":"2025-04-25T09:29:59.737993Z","shell.execute_reply.started":"2025-04-25T09:29:42.497965Z","shell.execute_reply":"2025-04-25T09:29:59.736961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\nclf.fit(X_train, y_train)\n\n# Validate\ny_pred = clf.predict(X_val)\nprint(\"Validation Accuracy:\", accuracy_score(y_val, y_pred))\nprint(classification_report(y_val, y_pred, target_names=label_map.keys()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:30:53.501937Z","iopub.execute_input":"2025-04-25T09:30:53.502243Z","iopub.status.idle":"2025-04-25T09:30:55.564862Z","shell.execute_reply.started":"2025-04-25T09:30:53.502221Z","shell.execute_reply":"2025-04-25T09:30:55.564094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test, y_test, _ = load_data(\"Tree_classification_augmented/test\")\ny_pred_test = clf.predict(X_test)\nprint(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:33:10.971282Z","iopub.execute_input":"2025-04-25T09:33:10.971665Z","iopub.status.idle":"2025-04-25T09:33:12.947642Z","shell.execute_reply.started":"2025-04-25T09:33:10.971639Z","shell.execute_reply":"2025-04-25T09:33:12.946887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\nimport os\n\ndef plot_augmented_samples(class_name=\"Acacia_Polycantha\", sample_img=\"tree1.jpg\"):\n    base_dir = \"Tree_classification_augmented\"\n    img_dir = os.path.join(base_dir, class_name)\n    \n    # Load original and augmented images\n    original = cv2.imread(os.path.join(img_dir, f\"{sample_img}\"))\n    rot90 = cv2.imread(os.path.join(img_dir, f\"{sample_img.split('.')[0]}_rot90.jpg\"))\n    tilt45 = cv2.imread(os.path.join(img_dir, f\"{sample_img.split('.')[0]}_tilt45.jpg\"))\n    flip = cv2.imread(os.path.join(img_dir, f\"{sample_img.split('.')[0]}_flip.jpg\"))\n    \n    # Convert BGR to RGB for matplotlib\n    images = {\n        \"Original\": cv2.cvtColor(original, cv2.COLOR_BGR2RGB),\n        \"Rotated 90°\": cv2.cvtColor(rot90, cv2.COLOR_BGR2RGB),\n        \"Tilted 45°\": cv2.cvtColor(tilt45, cv2.COLOR_BGR2RGB),\n        \"Horizontal Flip\": cv2.cvtColor(flip, cv2.COLOR_BGR2RGB)\n    }\n    \n    # Plot\n    fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n    for (title, img), ax in zip(images.items(), axes):\n        ax.imshow(img)\n        ax.set_title(title)\n        ax.axis('off')\n    plt.suptitle(f\"Augmentation Examples: {class_name}\", fontsize=16)\n    plt.show()\n\n# Example usage\nplot_augmented_samples(class_name=\"Acacia Polycantha(58)\", sample_img=\"Acacia Polycantha_10.jpg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:40:15.945002Z","iopub.execute_input":"2025-04-25T09:40:15.945319Z","iopub.status.idle":"2025-04-25T09:40:16.477498Z","shell.execute_reply.started":"2025-04-25T09:40:15.945279Z","shell.execute_reply":"2025-04-25T09:40:16.476513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ndef plot_confusion_matrix(y_true, y_pred, class_names):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n                xticklabels=class_names, yticklabels=class_names)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(\"Confusion Matrix\")\n    plt.xticks(rotation=45)\n    plt.yticks(rotation=0)\n    plt.show()\n\n# Example usage (after model training)\nclass_names = list(label_map.keys())\ny_pred_val = clf.predict(X_val)\nplot_confusion_matrix(y_val, y_pred_val, class_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:40:48.432337Z","iopub.execute_input":"2025-04-25T09:40:48.433197Z","iopub.status.idle":"2025-04-25T09:40:49.357250Z","shell.execute_reply.started":"2025-04-25T09:40:48.433171Z","shell.execute_reply":"2025-04-25T09:40:49.356419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_val, y_pred_val, target_names=class_names))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:41:21.007956Z","iopub.execute_input":"2025-04-25T09:41:21.008457Z","iopub.status.idle":"2025-04-25T09:41:21.021968Z","shell.execute_reply.started":"2025-04-25T09:41:21.008430Z","shell.execute_reply":"2025-04-25T09:41:21.021010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\n\n# Assume you've already trained `clf` and have `label_map`\njoblib.dump(clf, 'random_forest_model.joblib')  # Save the model\njoblib.dump(label_map, 'label_map.joblib')      # Save the class-label mapping\n\n# Verify files are saved\n!ls -l  # Check if files appear in Kaggle's output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T10:02:03.162836Z","iopub.execute_input":"2025-04-25T10:02:03.163559Z","iopub.status.idle":"2025-04-25T10:02:03.346419Z","shell.execute_reply.started":"2025-04-25T10:02:03.163526Z","shell.execute_reply":"2025-04-25T10:02:03.345425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the saved model and label map\nmodel = joblib.load('/kaggle/working/random_forest_model.joblib')  # or .pkl\nlabel_map = joblib.load('/kaggle/working/label_map.joblib')   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T10:04:00.170080Z","iopub.execute_input":"2025-04-25T10:04:00.171206Z","iopub.status.idle":"2025-04-25T10:04:00.235624Z","shell.execute_reply.started":"2025-04-25T10:04:00.171160Z","shell.execute_reply":"2025-04-25T10:04:00.234646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_and_show(image_path):\n    try:\n        # Preprocess + predict\n        img_processed = preprocess_image(image_path)\n        features = extract_features(img_processed).reshape(1, -1)\n        pred_label = model.predict(features)[0]\n        pred_class = list(label_map.keys())[list(label_map.values()).index(pred_label)]\n        \n        # Display\n        img = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n        plt.imshow(img)\n        plt.title(f\"Predicted: {pred_class}\")\n        plt.axis('off')\n        plt.show()\n        return pred_class\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return None\n\n# Example usage\ninput_image = '/kaggle/working/Tree_classification_augmented/Anant(53)/Anant_14_flip.jpg'  # Replace with your image path\npredicted_class = predict_and_show(input_image)\nprint(f\"Predicted Class: {predicted_class}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T10:06:25.964432Z","iopub.execute_input":"2025-04-25T10:06:25.964739Z","iopub.status.idle":"2025-04-25T10:06:26.181358Z","shell.execute_reply.started":"2025-04-25T10:06:25.964719Z","shell.execute_reply":"2025-04-25T10:06:26.180275Z"}},"outputs":[],"execution_count":null}]}